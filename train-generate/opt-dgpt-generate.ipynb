{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2c43f833-6e98-489f-9880-59d1dfc29792",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import pandas as pd\n",
    "import csv\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "# Add eos token\n",
    "def add_eos_for_dialogpt(prompt: str, eos_token: str) -> str:\n",
    "  \"\"\" Adds the end-of-sequence token (EOS) to each utterance \n",
    "      in the prompt, except the last one, for dialogpt.\n",
    "  \"\"\"\n",
    "  # Split the prompt by newline characters\n",
    "  utterances = prompt.strip().split('\\n')\n",
    "  \n",
    "  # Add EOS token to all but the last utterance\n",
    "  formatted_utterances = [u + eos_token for u in utterances[:-1]] + [utterances[-1]]\n",
    "  \n",
    "  # Join the formatted utterances with newline characters\n",
    "  return '\\n'.join(formatted_utterances)\n",
    "\n",
    "# Define function for generation \n",
    "def process_test_data(model_name, top_k=50):\n",
    "  # Read the test.csv into a pandas DataFrame\n",
    "  test = pd.read_csv('rephrase_test.csv', dtype={'prompt': str})\n",
    "  prompt = test['prompt'][0]\n",
    "\n",
    "  #test_sample = test.sample(5)\n",
    "  test_sample = test\n",
    "\n",
    "  # specify GPU\n",
    "  device = torch.device(\"cuda\")\n",
    "  if device is None:\n",
    "      device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "  print(f\"Using device: {device}\")\n",
    "\n",
    "  # Define base path for models\n",
    "  model_base_path = \"finetuned_models\"\n",
    "\n",
    "  # Iterate over each prompt in the list\n",
    "  for idx, row in test_sample.iterrows():\n",
    "      prompt = row['prompt']\n",
    "\n",
    "      # Conditionally reformat prompt based on model\n",
    "      if model_name == 'dialogpt':\n",
    "          eos_token = '<|endoftext|>'\n",
    "          prompt = add_eos_for_dialogpt(prompt, eos_token)\n",
    "\n",
    "      # Load model and tokenizer from specific folder\n",
    "      model_path = f\"{model_base_path}/{model_name}\"  # Construct model path\n",
    "      model = AutoModelForCausalLM.from_pretrained(model_path).to(device)\n",
    "      tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "      eos_token = tokenizer.eos_token_id \n",
    "\n",
    "      # Encode the prompt with EOS token\n",
    "      input_ids = tokenizer(prompt, return_tensors='pt', add_special_tokens=True)[\"input_ids\"].to(device)\n",
    "\n",
    "      # Generate responses with top-k sampling\n",
    "      output = model.generate(\n",
    "          input_ids=input_ids,\n",
    "          #max_length=100,\n",
    "          max_new_tokens=7,\n",
    "          num_return_sequences=5,\n",
    "          #no_repeat_ngram_size=2,\n",
    "          pad_token_id=tokenizer.eos_token_id,\n",
    "          do_sample=True,\n",
    "          top_k=top_k\n",
    "      )\n",
    "\n",
    "      # Decode and store the generated responses (temporary list)\n",
    "      generated_responses = []\n",
    "      for i in range(output.shape[0]):\n",
    "          generated_response = tokenizer.decode(output[i], skip_special_tokens=True, clean_up_tokenization_spaces=False)\n",
    "          generated_responses.append(generated_response)\n",
    "\n",
    "      # Last utterance by C (for overlap calculation)\n",
    "      last_utterance_by_c = prompt.split('\\n')[-2]\n",
    "\n",
    "      # Re-rank based on overlap with last utterance by C\n",
    "      def overlap_score(response):\n",
    "          response_words = set(response.split())\n",
    "          last_utterance_words = set(last_utterance_by_c.split())\n",
    "          return len(response_words.intersection(last_utterance_words))\n",
    "\n",
    "      ranked_responses = sorted(generated_responses, key=lambda x: overlap_score(x), reverse=True)\n",
    "\n",
    "      # Store the top 5 ranked responses\n",
    "      for i in range(5):\n",
    "          processed_response = ranked_responses[i].replace(prompt.replace('<|endoftext|>', ''), '').replace('\\n', ' ')\n",
    "          test_sample.at[idx, f'generated_response_{i+1}'] = processed_response\n",
    "\n",
    "  # Save the sample DataFrame to a CSV file\n",
    "  output_filename = f\"{model_name}_generated_responses.csv\"\n",
    "  test_sample.to_csv(output_filename, index=False)\n",
    "\n",
    "  print(f\"Generated responses saved to {output_filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b42641a-6b71-4b93-bff6-9bf0bfb7896c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate responses for opt model\n",
    "process_test_data('opt')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10 (tensorflow)",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
